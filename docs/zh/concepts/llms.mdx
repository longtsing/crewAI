---
title: 'LLMs'
description: '在您的 CrewAI 项目中配置和使用大语言模型（LLMs）的综合指南'
icon: 'microchip-ai'
mode: "wide"
---

## 概述

CrewAI 通过提供商的原生 SDK 与多个 LLM 提供商集成，为您提供灵活性来选择适合您特定用例的正确模型。本指南将帮助您了解如何在 CrewAI 项目中配置和使用不同的 LLM 提供商。

## 什么是 LLMs？

大语言模型（LLMs）是 CrewAI 代理背后的核心智能。它们使代理能够理解上下文、做出决策并生成类似人类的响应。以下是您需要了解的内容：

<CardGroup cols={2}>
  <Card title="LLM 基础" icon="brain">
    大语言模型是在大量文本数据上训练的 AI 系统。它们为您的 CrewAI 代理提供动力，使其能够理解和生成类似人类的文本。
  </Card>
  <Card title="上下文窗口" icon="window">
    上下文窗口决定了 LLM 一次可以处理多少文本。更大的窗口（例如 128K 令牌）允许更多上下文，但可能更昂贵且更慢。
  </Card>
  <Card title="温度" icon="temperature-three-quarters">
    温度（0.0 到 1.0）控制响应的随机性。较低的值（例如 0.2）产生更专注、确定性的输出，而较高的值（例如 0.8）增加创造力和可变性。
  </Card>
  <Card title="提供商选择" icon="server">
    每个 LLM 提供商（例如 OpenAI、Anthropic、Google）提供具有不同能力、定价和功能的模型。根据您对准确性、速度和成本的需求进行选择。
  </Card>
</CardGroup>

## 设置您的 LLM

在 CrewAI 代码中有不同的地方可以指定要使用的模型。一旦指定了您正在使用的模型，您将为每个使用的模型提供商提供配置（如 API 密钥）。请参阅[提供商配置示例](#provider-configuration-examples)部分了解您的提供商。

<Tabs>
  <Tab title="1. 环境变量">
    最简单的入门方法。直接在环境中设置模型，通过 `.env` 文件或在您的应用程序代码中设置。如果您使用 `crewai create` 来引导项目，它将已经被设置。

    ```bash .env
    MODEL=model-id  # 例如 gpt-4o, gemini-2.0-flash, claude-3-sonnet-...

    # 确保在此处也设置您的 API 密钥。请参阅提供商
    # 部分了解详细信息。
    ```

    <Warning>
      永远不要将 API 密钥提交到版本控制。使用环境文件（.env）或系统的秘密管理。
    </Warning>
  </Tab>
  <Tab title="2. YAML 配置">
    创建 YAML 文件来定义您的代理配置。这种方法非常适合版本控制和团队协作：

    ```yaml agents.yaml {6}
    researcher:
        role: 研究专家
        goal: 进行全面的研究和分析
        backstory: 具有多年经验的专职研究专业人员
        verbose: true
        llm: provider/model-id  # 例如 openai/gpt-4o, google/gemini-2.0-flash, anthropic/claude...
        # （更多详情请参阅下面的提供商配置示例）
    ```

    <Info>
      YAML 配置允许您：
      - 版本控制您的代理设置
      - 在不同模型之间轻松切换
      - 在团队成员之间共享配置
      - 记录模型选择及其用途
    </Info>
  </Tab>
  <Tab title="3. 直接代码">
    为了最大灵活性，直接在您的 Python 代码中配置 LLMs：

    ```python {4,8}
    from crewai import LLM

    # 基本配置
    llm = LLM(model="model-id-here")  # gpt-4o, gemini-2.0-flash, anthropic/claude...

    # 带有详细参数的高级配置
    llm = LLM(
        model="model-id-here",  # gpt-4o, gemini-2.0-flash, anthropic/claude...
        temperature=0.7,        # 更高值用于更有创意的输出
        timeout=120,            # 等待响应的秒数
        max_tokens=4000,        # 响应的最大长度
        top_p=0.9,              # 核采样参数
        frequency_penalty=0.1 , # 减少重复
        presence_penalty=0.1,   # 鼓励主题多样性
        response_format={"type": "json"},  # 用于结构化输出
        seed=42                 # 用于可重现的结果
    )
    ```

    <Info>
      参数说明：
      - `temperature`：控制随机性（0.0-1.0）
      - `timeout`：响应的最大等待时间
      - `max_tokens`：限制响应长度
      - `top_p`：采样的温度替代方案
      - `frequency_penalty`：减少词汇重复
      - `presence_penalty`：鼓励新主题
      - `response_format`：指定输出结构
      - `seed`：确保一致的输出
    </Info>
  </Tab>
</Tabs>

## 提供商配置示例

CrewAI 支持众多 LLM 提供商，每个提供商都提供独特的功能、身份验证方法和模型能力。
在本节中，您将找到详细的示例，帮助您选择、配置和优化最适合您项目需求的 LLM。

<AccordionGroup>
  <Accordion title="OpenAI">
    CrewAI 通过 OpenAI Python SDK 与 OpenAI 进行原生集成。

    ```toml Code
    # 必需
    OPENAI_API_KEY=sk-...

    # 可选
    OPENAI_BASE_URL=<custom-base-url>
    ```

    **基本用法：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="openai/gpt-4o",
        api_key="your-api-key",  # 或设置 OPENAI_API_KEY
        temperature=0.7,
        max_tokens=4000
    )
    ```

    **高级配置：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="openai/gpt-4o",
        api_key="your-api-key",
        base_url="https://api.openai.com/v1",  # 可选自定义端点
        organization="org-...",  # 可选组织 ID
        project="proj_...",  # 可选项目 ID
        temperature=0.7,
        max_tokens=4000,
        max_completion_tokens=4000,  # 用于较新模型
        top_p=0.9,
        frequency_penalty=0.1,
        presence_penalty=0.1,
        stop=["END"],
        seed=42,  # 用于可重现输出
        stream=True,  # 启用流式传输
        timeout=60.0,  # 请求超时时间（秒）
        max_retries=3,  # 最大重试次数
        logprobs=True,  # 返回对数概率
        top_logprobs=5,  # 最可能令牌的数量
        reasoning_effort="medium"  # 用于 o1 模型：low, medium, high
    )
    ```

    **结构化输出：**
    ```python Code
    from pydantic import BaseModel
    from crewai import LLM

    class ResponseFormat(BaseModel):
        name: str
        age: int
        summary: str

    llm = LLM(
        model="openai/gpt-4o",
    )
    ```

    **支持的环境变量：**
    - `OPENAI_API_KEY`：您的 OpenAI API 密钥（必需）
    - `OPENAI_BASE_URL`：OpenAI API 的自定义基础 URL（可选）

    **功能：**
    - 原生函数调用支持（o1 模型除外）
    - 使用 JSON 模式的结构化输出
    - 实时响应的流式传输支持
    - 令牌使用跟踪
    - 停止序列支持（o1 模型除外）
    - 令牌级别洞察的对数概率
    - o1 模型的推理控制

    **支持的模型：**

    | 模型               | 上下文窗口   | 最适用于                                      |
    |---------------------|------------------|-----------------------------------------------|
    | gpt-4.1             | 1M 令牌        | 具有增强功能的最新模型                       |
    | gpt-4.1-mini        | 1M 令牌        | 具有大上下文的高效版本                        |
    | gpt-4.1-nano        | 1M 令牌        | 超高效变体                                 |
    | gpt-4o              | 128,000 令牌   | 为速度和智能优化                              |
    | gpt-4o-mini         | 200,000 令牌   | 具有大上下文的成本效益                      |
    | gpt-4-turbo         | 128,000 令牌   | 长篇内容、文档分析                          |
    | gpt-4               | 8,192 令牌     | 高精度任务、复杂推理                          |
    | o1                  | 200,000 令牌   | 高级推理、复杂问题解决                        |
    | o1-preview          | 128,000 令牌   | 推理功能预览                                |
    | o1-mini             | 128,000 令牌   | 高效推理模型                                 |
    | o3-mini             | 200,000 令牌   | 轻量级推理模型                              |
    | o4-mini             | 200,000 令牌   | 下一代高效推理                               |

    **注意：**要使用 OpenAI，请安装所需的依赖项：
    ```bash
    uv add "crewai[openai]"
    ```
  </Accordion>

  <Accordion title="Meta-Llama">
    Meta 的 Llama API 提供对 Meta 大语言模型家族的访问。
    该 API 通过 [Meta Llama API](https://llama.developer.meta.com?utm_source=partner-crewai&utm_medium=website) 提供。
    在您的 `.env` 文件中设置以下环境变量：

    ```toml Code
    # Meta Llama API 密钥配置
    LLAMA_API_KEY=LLM|your_api_key_here
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    from crewai import LLM

    # 初始化 Meta Llama LLM
    llm = LLM(
        model="meta_llama/Llama-4-Scout-17B-16E-Instruct-FP8",
        temperature=0.8,
        stop=["END"],
        seed=42
    )
    ```

    此处列出的所有模型都受支持 https://llama.developer.meta.com/docs/models/。

    | 模型 ID | 输入上下文长度 | 输出上下文长度 | 输入模态 | 输出模态 |
    | --- | --- | --- | --- | --- |
    | `meta_llama/Llama-4-Scout-17B-16E-Instruct-FP8` | 128k | 4028 | 文本、图像 | 文本 |
    | `meta_llama/Llama-4-Maverick-17B-128E-Instruct-FP8` | 128k | 4028 | 文本、图像 | 文本 |
    | `meta_llama/Llama-3.3-70B-Instruct` | 128k | 4028 | 文本 | 文本 |
    | `meta_llama/Llama-3.3-8B-Instruct` | 128k | 4028 | 文本 | 文本 |
  </Accordion>

  <Accordion title="Anthropic">
    CrewAI 通过 Anthropic Python SDK 与 Anthropic 进行原生集成。

    ```toml Code
    # 必需
    ANTHROPIC_API_KEY=sk-ant-...
    ```

    **基本用法：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="anthropic/claude-3-5-sonnet-20241022",
        api_key="your-api-key",  # 或设置 ANTHROPIC_API_KEY
        max_tokens=4096  # Anthropic 必需
    )
    ```

    **高级配置：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="anthropic/claude-3-5-sonnet-20241022",
        api_key="your-api-key",
        base_url="https://api.anthropic.com",  # 可选自定义端点
        temperature=0.7,
        max_tokens=4096,  # 必需参数
        top_p=0.9,
        stop_sequences=["END", "STOP"],  # Anthropic 使用 stop_sequences
        stream=True,  # 启用流式传输
        timeout=60.0,  # 请求超时时间（秒）
        max_retries=3  # 最大重试次数
    )
    ```

    **支持的环境变量：**
    - `ANTHROPIC_API_KEY`：您的 Anthropic API 密钥（必需）

    **功能：**
    - Claude 3+ 模型的原生工具使用支持
    - 实时响应的流式传输支持
    - 自动系统消息处理
    - 受控输出的停止序列
    - 令牌使用跟踪
    - 多轮工具使用对话

    **重要说明：**
    - `max_tokens` 是所有 Anthropic 模型的**必需**参数
    - Claude 使用 `stop_sequences` 而不是 `stop`
    - 系统消息与对话消息分开处理
    - 第一条消息必须来自用户（自动处理）
    - 消息必须在用户和助手之间交替

    **支持的模型：**

    | 模型                        | 上下文窗口 | 最适用于                                      |
    |------------------------------|----------------|-----------------------------------------------|
    | claude-3-7-sonnet            | 200,000 令牌 | 高级推理和代理任务                            |
    | claude-3-5-sonnet-20241022   | 200,000 令牌 | 具有最佳性能的最新 Sonnet                   |
    | claude-3-5-haiku             | 200,000 令牌 | 快速、紧凑的模型用于快速响应                 |
    | claude-3-opus                | 200,000 令牌 | 复杂任务最强大的模型                         |
    | claude-3-sonnet              | 200,000 令牌 | 平衡的智能和速度                             |
    | claude-3-haiku               | 200,000 令牌 | 简单任务最快的模型                           |
    | claude-2.1                   | 200,000 令牌 | 扩展上下文，减少幻觉                         |
    | claude-2                     | 100,000 令牌 | 各种任务的通用模型                            |
    | claude-instant               | 100,000 令牌 | 日常任务快速、经济高效的模型                 |

    **注意：**要使用 Anthropic，请安装所需的依赖项：
    ```bash
    uv add "crewai[anthropic]"
    ```
  </Accordion>

  <Accordion title="Google (Gemini API)">
    CrewAI 通过 Google Gen AI Python SDK 与 Google Gemini 进行原生集成。

    在您的 `.env` 文件中设置您的 API 密钥。如果您需要密钥，请查看 [AI Studio](https://aistudio.google.com/apikey)。

    ```toml .env
    # 必需（以下之一）
    GOOGLE_API_KEY=<your-api-key>
    GEMINI_API_KEY=<your-api-key>

    # 可选 - 用于 Vertex AI
    GOOGLE_CLOUD_PROJECT=<your-project-id>
    GOOGLE_CLOUD_LOCATION=<location>  # 默认为 us-central1
    GOOGLE_GENAI_USE_VERTEXAI=true  # 设置为使用 Vertex AI
    ```

    **基本用法：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="gemini/gemini-2.0-flash",
        api_key="your-api-key",  # 或设置 GOOGLE_API_KEY/GEMINI_API_KEY
        temperature=0.7
    )
    ```

    **高级配置：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="gemini/gemini-2.5-flash",
        api_key="your-api-key",
        temperature=0.7,
        top_p=0.9,
        top_k=40,  # Top-k 采样参数
        max_output_tokens=8192,
        stop_sequences=["END", "STOP"],
        stream=True,  # 启用流式传输
        safety_settings={
            "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
            "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE"
        }
    )
    ```

    **Vertex AI 配置：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="gemini/gemini-1.5-pro",
        project="your-gcp-project-id",
        location="us-central1"  # GCP 区域
    )
    ```

    **支持的环境变量：**
    - `GOOGLE_API_KEY` 或 `GEMINI_API_KEY`：您的 Google API 密钥（Gemini API 必需）
    - `GOOGLE_CLOUD_PROJECT`：Google Cloud 项目 ID（用于 Vertex AI）
    - `GOOGLE_CLOUD_LOCATION`：GCP 位置（默认为 `us-central1`）
    - `GOOGLE_GENAI_USE_VERTEXAI`：设置为 `true` 以使用 Vertex AI

    **功能：**
    - Gemini 1.5+ 和 2.x 模型的原生函数调用支持
    - 实时响应的流式传输支持
    - 多模态能力（文本、图像、视频）
    - 安全设置配置
    - 支持 Gemini API 和 Vertex AI
    - 自动系统指令处理
    - 令牌使用跟踪

    **Gemini 模型：**

    Google 提供一系列为不同用例优化的强大模型。

    | 模型                          | 上下文窗口 | 最适用于                                                          |
    |--------------------------------|----------------|-------------------------------------------------------------------|
    | gemini-2.5-flash               | 1M 令牌      | 自适应思维、成本效益                              |
    | gemini-2.5-pro                 | 1M 令牌      | 增强思维和推理、多模态理解                 |
    | gemini-2.0-flash               | 1M 令牌      | 下一代功能、速度、思维                        |
    | gemini-2.0-flash-thinking      | 32,768 令牌  | 具有思维过程的高级推理                        |
    | gemini-2.0-flash-lite          | 1M 令牌      | 成本效益和低延迟                                   |
    | gemini-1.5-pro                 | 2M 令牌      | 最佳性能、逻辑推理、编码                        |
    | gemini-1.5-flash               | 1M 令牌      | 平衡的多模态模型，适用于大多数任务                 |
    | gemini-1.5-flash-8b            | 1M 令牌      | 最快、最具成本效益                                  |
    | gemini-1.0-pro                 | 32,768 令牌  | 上一代模型                                          |

    **Gemma 模型：**

    Gemini API 还支持托管在 Google 基础设施上的 [Gemma 模型](https://ai.google.dev/gemma/docs)。

    | 模型          | 上下文窗口 | 最适用于                           |
    |----------------|----------------|------------------------------------|
    | gemma-3-1b     | 32,000 令牌  | 超轻量级任务            |
    | gemma-3-4b     | 128,000 令牌 | 高效的通用任务    |
    | gemma-3-12b    | 128,000 令牌 | 平衡的性能和效率|
    | gemma-3-27b    | 128,000 令牌 | 高性能任务         |

    **注意：**要使用 Google Gemini，请安装所需的依赖项：
    ```bash
    uv add "crewai[google-genai]"
    ```

    完整的模型列表可在 [Gemini 模型文档](https://ai.google.dev/gemini-api/docs/models) 中找到。
  </Accordion>
  <Accordion title="Google (Vertex AI)">
    从您的 Google Cloud 控制台获取凭据并将其保存到 JSON 文件，然后使用以下代码加载：
    ```python Code
    import json

    file_path = 'path/to/vertex_ai_service_account.json'

    # 加载 JSON 文件
    with open(file_path, 'r') as file:
        vertex_credentials = json.load(file)

    # 将凭据转换为 JSON 字符串
    vertex_credentials_json = json.dumps(vertex_credentials)
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="gemini-1.5-pro-latest", # 或 vertex_ai/gemini-1.5-pro-latest
        temperature=0.7,
        vertex_credentials=vertex_credentials_json
    )
    ```

    Google 提供一系列为不同用例优化的强大模型：

    | 模型                          | 上下文窗口 | 最适用于                                                          |
    |--------------------------------|----------------|-------------------------------------------------------------------|
    | gemini-2.5-flash-preview-04-17 | 1M 令牌      | 自适应思维、成本效益                                |
    | gemini-2.5-pro-preview-05-06   | 1M 令牌      | 增强思维和推理、多模态理解、高级编码等               |
    | gemini-2.0-flash               | 1M 令牌      | 下一代功能、速度、思维和实时流式传输 |
    | gemini-2.0-flash-lite          | 1M 令牌      | 成本效益和低延迟                                   |
    | gemini-1.5-flash               | 1M 令牌      | 平衡的多模态模型，适用于大多数任务                    |
    | gemini-1.5-flash-8B            | 1M 令牌      | 最快、最具成本效益，适用于高频任务               |
    | gemini-1.5-pro                 | 2M 令牌      | 最佳性能，各种推理任务包括逻辑推理、编码和创意协作 |
  </Accordion>

  <Accordion title="Azure">
    CrewAI 通过 Azure AI Inference Python SDK 与 Azure AI Inference 和 Azure OpenAI 进行原生集成。

    ```toml Code
    # 必需
    AZURE_API_KEY=<your-api-key>
    AZURE_ENDPOINT=<your-endpoint-url>

    # 可选
    AZURE_API_VERSION=<api-version>  # 默认为 2024-06-01
    ```

    **端点 URL 格式：**

    对于 Azure OpenAI 部署：
    ```
    https://<resource-name>.openai.azure.com/openai/deployments/<deployment-name>
    ```

    对于 Azure AI Inference 端点：
    ```
    https://<resource-name>.inference.azure.com
    ```

    **基本用法：**
    ```python Code
    llm = LLM(
        model="azure/gpt-4",
        api_key="<your-api-key>",  # 或设置 AZURE_API_KEY
        endpoint="<your-endpoint-url>",
        api_version="2024-06-01"
    )
    ```

    **高级配置：**
    ```python Code
    llm = LLM(
        model="azure/gpt-4o",
        temperature=0.7,
        max_tokens=4000,
        top_p=0.9,
        frequency_penalty=0.0,
        presence_penalty=0.0,
        stop=["END"],
        stream=True,
        timeout=60.0,
        max_retries=3
    )
    ```

    **支持的环境变量：**
    - `AZURE_API_KEY`：您的 Azure API 密钥（必需）
    - `AZURE_ENDPOINT`：您的 Azure 端点 URL（必需，也检查 `AZURE_OPENAI_ENDPOINT` 和 `AZURE_API_BASE`）
    - `AZURE_API_VERSION`：API 版本（可选，默认为 `2024-06-01`）

    **功能：**
    - Azure OpenAI 模型（gpt-4、gpt-4o、gpt-3.5-turbo 等）的原生函数调用支持
    - 实时响应的流式传输支持
    - 自动端点 URL 验证和纠正
    - 带有重试逻辑的综合错误处理
    - 令牌使用跟踪

    **注意：**要使用 Azure AI Inference，请安装所需的依赖项：
    ```bash
    uv add "crewai[azure-ai-inference]"
    ```
  </Accordion>

  <Accordion title="AWS Bedrock">
    CrewAI 通过使用 Converse API 的 boto3 SDK 与 AWS Bedrock 进行原生集成。

    ```toml Code
    # 必需
    AWS_ACCESS_KEY_ID=<your-access-key>
    AWS_SECRET_ACCESS_KEY=<your-secret-key>

    # 可选
    AWS_SESSION_TOKEN=<your-session-token>  # 用于临时凭据
    AWS_DEFAULT_REGION=<your-region>  # 默认为 us-east-1
    ```

    **基本用法：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0",
        region_name="us-east-1"
    )
    ```

    **高级配置：**
    ```python Code
    from crewai import LLM

    llm = LLM(
        model="bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0",
        aws_access_key_id="your-access-key",  # 或设置 AWS_ACCESS_KEY_ID
        aws_secret_access_key="your-secret-key",  # 或设置 AWS_SECRET_ACCESS_KEY
        aws_session_token="your-session-token",  # 用于临时凭据
        region_name="us-east-1",
        temperature=0.7,
        max_tokens=4096,
        top_p=0.9,
        top_k=250,  # 用于 Claude 模型
        stop_sequences=["END", "STOP"],
        stream=True,  # 启用流式传输
        guardrail_config={  # 可选内容过滤
            "guardrailIdentifier": "your-guardrail-id",
            "guardrailVersion": "1"
        },
        additional_model_request_fields={  # 模型特定参数
            "top_k": 250
        }
    )
    ```

    **支持的环境变量：**
    - `AWS_ACCESS_KEY_ID`：AWS 访问密钥（必需）
    - `AWS_SECRET_ACCESS_KEY`：AWS 秘密密钥（必需）
    - `AWS_SESSION_TOKEN`：AWS 会话令牌用于临时凭据（可选）
    - `AWS_DEFAULT_REGION`：AWS 区域（默认为 `us-east-1`）

    **功能：**
    - 通过 Converse API 的原生工具调用支持
    - 流式传输和非流式传输响应
    - 带有重试逻辑的综合错误处理
    - 内容过滤的防护栏配置
    - 通过 `additional_model_request_fields` 的模型特定参数
    - 令牌使用跟踪和停止原因日志记录
    - 支持所有 Bedrock 基础模型
    - 自动对话格式处理

    **重要说明：**
    - 使用现代 Converse API 进行统一的模型访问
    - 自动处理模型特定的对话要求
    - 系统消息与对话分开处理
    - 第一条消息必须来自用户（自动处理）
    - 一些模型（如 Cohere）要求对话以用户消息结束

    [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) 是一个托管服务，通过统一的 API 提供对顶级 AI 公司多个基础模型的访问。

    | 模型                   | 上下文窗口       | 最适用于                                                          |
    |-------------------------|------------------|-------------------------------------------------------------------|
    | Amazon Nova Pro         | 最多 300k 令牌    | 高性能、模型平衡准确性、速度和成本效益，适用于各种任务。 |
    | Amazon Nova Micro       | 最多 128k 令牌    | 高性能、经济高效的纯文本模型，优化为最低延迟响应。 |
    | Amazon Nova Lite        | 最多 300k 令牌    | 高性能、经济实惠的多模态处理，适用于图像、视频和文本，具有实时能力。 |
    | Claude 3.7 Sonnet       | 最多 128k 令牌    | 高性能，最适用于复杂推理、编码和 AI 代理 |
    | Claude 3.5 Sonnet v2    | 最多 200k 令牌    | 最先进模型，专门从事软件工程、代理能力和计算机交互，具有优化成本。 |
    | Claude 3.5 Sonnet       | 最多 200k 令牌    | 高性能模型，在各种任务中提供卓越的智能和推理，具有最佳速度成本平衡。 |
    | Claude 3.5 Haiku        | 最多 200k 令牌    | 快速、紧凑的多模态模型，优化为快速响应和无缝人类类交互 |
    | Claude 3 Sonnet         | 最多 200k 令牌    | 平衡智能和速度的多模态模型，用于高容量部署。 |
    | Claude 3 Haiku          | 最多 200k 令牌    | 紧凑、高速的多模态模型，优化为快速响应和自然对话交互 |
    | Claude 3 Opus           | 最多 200k 令牌    | 最先进的多模态模型，在复杂任务中表现出色，具有人类类推理和卓越的上下文理解。 |
    | Claude 2.1              | 最多 200k 令牌    | 增强版本，具有扩展的上下文窗口、改进的可靠性和减少的幻觉，适用于长篇和 RAG 应用 |
    | Claude                  | 最多 100k 令牌    | 通用模型，擅长复杂对话、创意内容和精确指令遵循。 |
    | Claude Instant          | 最多 100k 令牌    | 快速、经济高效的模型，用于日常任务，如对话、分析、摘要和文档问答 |
    | Llama 3.1 405B Instruct | 最多 128k 令牌    | 高级 LLM，用于合成数据生成、蒸馏和推理，适用于聊天机器人、编码和领域特定任务。 |
    | Llama 3.1 70B Instruct  | 最多 128k 令牌    | 驱动复杂对话，具有卓越的上下文理解、推理和文本生成。 |
    | Llama 3.1 8B Instruct   | 最多 128k 令牌    | 先进的状态-of-the-art 模型，具有语言理解、卓越的推理和文本生成。 |
    | Llama 3 70B Instruct    | 最多 8k 令牌      | 驱动复杂对话，具有卓越的上下文理解、推理和文本生成。 |
    | Llama 3 8B Instruct     | 最多 8k 令牌      | 先进的状态-of-the-art LLM，具有语言理解、卓越的推理和文本生成。 |
    | Titan Text G1 - Lite    | 最多 4k 令牌      | 轻量级、经济高效的模型，优化为英语任务和微调，专注于摘要和内容生成。 |
    | Titan Text G1 - Express | 最多 8k 令牌      | 通用语言任务的通用模型，支持英语和 100+ 种语言的聊天和 RAG 应用。 |
    | Cohere Command          | 最多 4k 令牌      | 专门遵循用户命令并提供实用企业解决方案的模型。 |
    | Jurassic-2 Mid          | 最多 8,191 令牌   | 平衡质量和经济性的成本效益模型，适用于各种语言任务，如问答、摘要和内容生成。 |
    | Jurassic-2 Ultra        | 最多 8,191 令牌   | 高级文本生成和理解模型，在分析和内容创建等复杂任务中表现出色。 |
    | Jamba-Instruct          | 最多 256k 令牌    | 具有扩展上下文窗口的模型，优化为经济高效的文本生成、摘要和问答。 |
    | Mistral 7B Instruct     | 最多 32k 令牌     | 此 LLM 遵循指令、完成请求并生成创意文本。 |
    | Mistral 8x7B Instruct   | 最多 32k 令牌     | 一个遵循指令、完成请求并生成创意文本的 MOE LLM。 |
    | DeepSeek R1             | 32,768 令牌        | 高级推理模型                                                       |

    **注意：**要使用 AWS Bedrock，请安装所需的依赖项：
    ```bash
    uv add "crewai[bedrock]"
    ```
  </Accordion>

  <Accordion title="Amazon SageMaker">
    ```toml Code
    AWS_ACCESS_KEY_ID=<your-access-key>
    AWS_SECRET_ACCESS_KEY=<your-secret-key>
    AWS_DEFAULT_REGION=<your-region>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="sagemaker/<my-endpoint>"
    )
    ```
  </Accordion>

  <Accordion title="Mistral">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    MISTRAL_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="mistral/mistral-large-latest",
        temperature=0.7
    )
    ```
  </Accordion>

  <Accordion title="Nvidia NIM">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    NVIDIA_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="nvidia_nim/meta/llama3-70b-instruct",
        temperature=0.7
    )
    ```

    Nvidia NIM 为各种用例提供全面的模型套件，从通用任务到专门应用。

    | 模型                                                                   | 上下文窗口 | 最适用于                                                          |
    |-------------------------------------------------------------------------|----------------|-------------------------------------------------------------------|
    | nvidia/mistral-nemo-minitron-8b-8k-instruct                              | 8,192 令牌   | 为聊天机器人、虚拟助手和内容生成提供卓越准确性的最新小型语言模型。 |
    | nvidia/nemotron-4-mini-hindi-4b-instruct                                 | 4,096 令牌   | 双语印地语-英语 SLM，用于设备端推理，专门为印地语语言定制。 |
    | nvidia/llama-3.1-nemotron-70b-instruct                                  | 128k 令牌    | 定制为增强响应的帮助性                  |
    | nvidia/llama3-chatqa-1.5-8b                                                | 128k 令牌    | 高级 LLM，为聊天机器人和搜索引擎生成高质量、上下文感知的响应。 |
    | nvidia/llama3-chatqa-1.5-70b                                               | 128k 令牌    | 高级 LLM，为聊天机器人和搜索引擎生成高质量、上下文感知的响应。 |
    | nvidia/vila                                                             | 128k 令牌    | 理解文本/图像/视频并创建信息响应的多模态视觉语言模型 |
    | nvidia/neva-22                                                          | 4,096 令牌   | 理解文本/图像并生成信息响应的多模态视觉语言模型 |
    | nvidia/nemotron-mini-4b-instruct                                         | 8,192 令牌   | 通用任务 |
    | nvidia/usdcode-llama3-70b-instruct                                       | 128k 令牌    | 回答 OpenUSD 知识查询并生成 USD-Python 代码的最新 LLM。 |
    | nvidia/nemotron-4-340b-instruct                                          | 4,096 令牌   | 创建模拟现实世界特征的各种合成数据。 |
    | meta/codellama-70b                                                      | 100k 令牌    | 能够从自然语言生成代码反之亦然的 LLM。 |
    | meta/llama2-70b                                                         | 4,096 令牌   | 能够生成文本和代码以响应提示的最新大语言 AI 模型。 |
    | meta/llama3-8b-instruct                                                | 8,192 令牌   | 具有语言理解、卓越推理和文本生成的最新状态-of-the-art LLM。 |
    | meta/llama3-70b-instruct                                               | 8,192 令牌   | 驱动复杂对话，具有卓越的上下文理解、推理和文本生成。 |
    | meta/llama-3.1-8b-instruct                                             | 128k 令牌    | 具有语言理解、卓越推理和文本生成的最新状态-of-the-art 模型。 |
    | meta/llama-3.1-70b-instruct                                            | 128k 令牌    | 驱动复杂对话，具有卓越的上下文理解、推理和文本生成。 |
    | meta/llama-3.1-405b-instruct                                           | 128k 令牌    | 高级 LLM，用于合成数据生成、蒸馏和推理，适用于聊天机器人、编码和领域特定任务。 |
    | meta/llama-3.2-1b-instruct                                             | 128k 令牌    | 具有语言理解、卓越推理和文本生成的最新状态-of-the-art 小型语言模型。 |
    | meta/llama-3.2-3b-instruct                                             | 128k 令牌    | 具有语言理解、卓越推理和文本生成的最新状态-of-the-art 小型语言模型。 |
    | meta/llama-3.2-11b-vision-instruct                                     | 128k 令牌    | 具有语言理解、卓越推理和文本生成的最新状态-of-the-art 小型语言模型。 |
    | meta/llama-3.2-90b-vision-instruct                                     | 128k 令牌    | 具有语言理解、卓越推理和文本生成的最新状态-of-the-art 小型语言模型。 |
    | google/gemma-7b                                                        | 8,192 令牌   | 最新的文本生成模型，文本理解、转换和编码生成。 |
    | google/gemma-2b                                                        | 8,192 令牌   | 最新的文本生成模型，文本理解、转换和编码生成。 |
    | google/codegemma-7b                                                    | 8,192 令牌   | 基于 Google 的 Gemma-7B 专门用于编码生成和代码完成的最新模型。 |
    | google/codegemma-1.1-7b                                               | 8,192 令牌   | 高级编程模型，用于编码生成、完成、推理和指令遵循。 |
    | google/recurrentgemma-2b                                              | 8,192 令牌   | 用于生成长序列时更快推理的新型循环架构语言模型。 |
    | google/gemma-2-9b-it                                                  | 8,192 令牌   | 最新的文本生成模型，文本理解、转换和编码生成。 |
    | google/gemma-2-27b-it                                                 | 8,192 令牌   | 最新的文本生成模型，文本理解、转换和编码生成。 |
    | google/gemma-2-2b-it                                                  | 8,192 令牌   | 最新的文本生成模型，文本理解、转换和编码生成。 |
    | google/deplot                                                         | 512 令牌     | 一次性视觉语言理解模型，将绘图图像转换为表格。 |
    | google/paligemma                                                      | 8,192 令牌   | 擅长理解文本和视觉输入以产生信息响应的视觉语言模型。 |
    | mistralai/mistral-7b-instruct-v0.2                                   | 32k 令牌     | 此 LLM 遵循指令、完成请求并生成创意文本。 |
    | mistralai/mixtral-8x7b-instruct-v0.1                                 | 8,192 令牌   | 遵循指令、完成请求并生成创意文本的 MOE LLM。 |
    | mistralai/mistral-large                                              | 4,096 令牌   | 创建模拟现实世界特征的各种合成数据。 |
    | mistralai/mixtral-8x22b-instruct-v0.1                               | 8,192 令牌   | 创建模拟现实世界特征的各种合成数据。 |
    | mistralai/mistral-7b-instruct-v0.3                                  | 32k 令牌     | 此 LLM 遵循指令、完成请求并生成创意文本。 |
    | nv-mistralai/mistral-nemo-12b-instruct                              | 128k 令牌    | 最先进的语言模型，用于推理、编码、多语言任务；在单个 GPU 上运行。 |
    | mistralai/mamba-codestral-7b-v0.1                                   | 256k 令牌    | 用于在各种编程语言和任务中编写和交互代码的模型。 |
    | microsoft/phi-3-mini-128k-instruct                                  | 128K 令牌    | 轻量级、最新状态的开放 LLM，具有强大的数学和逻辑推理技能。 |
    | microsoft/phi-3-mini-4k-instruct                                    | 4,096 令牌   | 轻量级、最新状态的开放 LLM，具有强大的数学和逻辑推理技能。 |
    | microsoft/phi-3-small-8k-instruct                                   | 8,192 令牌   | 轻量级、最新状态的开放 LLM，具有强大的数学和逻辑推理技能。 |
    | microsoft/phi-3-small-128k-instruct                                 | 128K 令牌    | 轻量级、最新状态的开放 LLM，具有强大的数学和逻辑推理技能。 |
    | microsoft/phi-3-medium-4k-instruct                                  | 4,096 令牌   | 轻量级、最新状态的开放 LLM，具有强大的数学和逻辑推理技能。 |
    | microsoft/phi-3-medium-128k-instruct                                | 128K 令牌    | 轻量级、最新状态的开放 LLM，具有强大的数学和逻辑推理技能。 |
    | microsoft/phi-3.5-mini-instruct                                     | 128K 令牌    | 轻量级多语言 LLM，在延迟受限、内存/计算受限环境中驱动 AI 应用 |
    | microsoft/phi-3.5-moe-instruct                                      | 128K 令牌    | 基于 Mixture of Experts 架构的高级 LLM，提供计算高效的内容生成 |
    | microsoft/kosmos-2                                                  | 1,024 令牌   | 旨在理解和推理图像中视觉元素的开创性多模态模型。 |
    | microsoft/phi-3-vision-128k-instruct                               | 128k 令牌    | 在从图像进行高质量推理方面表现卓越的最新开放多模态模型。 |
    | microsoft/phi-3.5-vision-instruct                                  | 128k 令牌    | 在从图像进行高质量推理方面表现卓越的最新开放多模态模型。 |
    | databricks/dbrx-instruct                                           | 12k 令牌     | 在语言理解、编码和 RAG 方面具有最新性能的通用 LLM。 |
    | snowflake/arctic                                                   | 1,024 令牌   | 为专注于 SQL 生成和编码的企业应用程序提供高效推理。 |
    | aisingapore/sea-lion-7b-instruct                                  | 4,096 令牌   | 代表和服務东南亚语言和文化多样性的 LLM |
    | ibm/granite-8b-code-instruct                                      | 4,096 令牌   | 软件编程 LLM，用于代码生成、完成、解释和多轮转换。 |
    | ibm/granite-34b-code-instruct                                     | 8,192 令牌   | 软件编程 LLM，用于代码生成、完成、解释和多轮转换。 |
    | ibm/granite-3.0-8b-instruct                                       | 4,096 令牌   | 支持RAG、摘要、分类、编码和代理AI 的高级小型语言模型 |
    | ibm/granite-3.0-3b-a800m-instruct                                | 4,096 令牌   | 高效的 Mixture of Experts 模型，用于 RAG、摘要、实体提取和分类 |
    | mediatek/breeze-7b-instruct                                       | 4,096 令牌   | 创建模拟现实世界特征的各种合成数据。 |
    | upstage/solar-10.7b-instruct                                      | 4,096 令牌   | 在 NLP 任务中表现出色，特别是在指令遵循、推理和数学方面。 |
    | writer/palmyra-med-70b-32k                                        | 32k 令牌     | 在医疗领域提供准确、上下文相关响应的领先 LLM。 |
    | writer/palmyra-med-70b                                            | 32k 令牌     | 在医疗领域提供准确、上下文相关响应的领先 LLM。 |
    | writer/palmyra-fin-70b-32k                                        | 32k 令牌     | 专门用于金融分析、报告和数据处理的专用 LLM |
    | 01-ai/yi-large                                                    | 32k 令牌     | 在英语和中文上训练的强大模型，用于包括聊天机器人和创意写作在内的各种任务。 |
    | deepseek-ai/deepseek-coder-6.7b-instruct                         | 2k 令牌      | 提供高级编码功能的强大编码模型 |
    | rakuten/rakutenai-7b-instruct                                     | 1,024 令牌   | 具有语言理解、卓越推理和文本生成的最新状态-of-the-art LLM。 |
    | rakuten/rakutenai-7b-chat                                         | 1,024 令牌   | 具有语言理解、卓越推理和文本生成的最新状态-of-the-art LLM。 |
    | baichuan-inc/baichuan2-13b-chat                                  | 4,096 令牌   | 支持中文和英文聊天、编码、数学、指令遵循、解决测验 |
  </Accordion>

  <Accordion title="本地 NVIDIA NIM 使用 WSL2 部署">

    NVIDIA NIM 使您能够使用 WSL2（Windows Subsystem for Linux）在您的 Windows 机器上运行强大的 LLM 本地。
    这种方法允许您利用您的 NVIDIA GPU 进行私有、安全和成本效益的 AI 推理，而不依赖云服务。
    非常适合开发、测试或需要数据隐私或离线功能的生产场景。

    以下是设置本地 NVIDIA NIM 模型的分步指南：

    1. 遵循 [NVIDIA 网站](https://docs.nvidia.com/nim/wsl2/latest/getting-started.html) 的安装说明

    2. 安装本地模型。对于 Llama 3.1-8b 遵循 [说明](https://build.nvidia.com/meta/llama-3_1-8b-instruct/deploy)

    3. 配置您的 crewai 本地模型：

    ```python Code
    from crewai.llm import LLM

    local_nvidia_nim_llm = LLM(
        model="openai/meta/llama-3.1-8b-instruct", # 它是一个 openai-api 兼容模型
        base_url="http://localhost:8000/v1",
        api_key="<your_api_key|any text if you have not configured it>", # api_key 是必需的，但您可以使用任何文本
    )

    # 然后您可以在您的 crew 中使用它：

    @CrewBase
    class MyCrew():
        # ...

        @agent
        def researcher(self) -> Agent:
            return Agent(
                config=self.agents_config['researcher'], # type: ignore[index]
                llm=local_nvidia_nim_llm
            )

        # ...
    ```
  </Accordion>

  <Accordion title="Groq">
    在您的 `.env` 文件中设置以下环境变量：

    ```toml Code
    GROQ_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="groq/llama-3.2-90b-text-preview",
        temperature=0.7
    )
    ```
    | 模型             | 上下文窗口   | 最适用于                                   |
    |-------------------|------------------|--------------------------------------------|
    | Llama 3.1 70B/8B  | 131,072 令牌   | 高性能、大上下文任务      |
    | Llama 3.2 Series  | 8,192 令牌     | 通用任务                      |
    | Mixtral 8x7B      | 32,768 令牌    | 平衡性能和上下文           |
  </Accordion>

  <Accordion title="IBM watsonx.ai">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    # 必需
    WATSONX_URL=<your-url>
    WATSONX_APIKEY=<your-apikey>
    WATSONX_PROJECT_ID=<your-project-id>

    # 可选
    WATSONX_TOKEN=<your-token>
    WATSONX_DEPLOYMENT_SPACE_ID=<your-space-id>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="watsonx/meta-llama/llama-3-1-70b-instruct",
        base_url="https://api.watsonx.ai/v1"
    )
    ```
  </Accordion>

  <Accordion title="Ollama (本地 LLMs)">
    1. 安装 Ollama：[ollama.ai](https://ollama.ai/)
    2. 运行模型：`ollama run llama3`
    3. 配置：

    ```python Code
    llm = LLM(
        model="ollama/llama3:70b",
        base_url="http://localhost:11434"
    )
    ```
  </Accordion>

  <Accordion title="Fireworks AI">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    FIREWORKS_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct",
        temperature=0.7
    )
    ```
  </Accordion>

  <Accordion title="Perplexity AI">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    PERPLEXITY_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="llama-3.1-sonar-large-128k-online",
        base_url="https://api.perplexity.ai/"
    )
    ```
  </Accordion>

  <Accordion title="Hugging Face">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    HF_TOKEN=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct"
    )
    ```
  </Accordion>

  <Accordion title="SambaNova">
    在您的 `.env` 文件中设置以下环境变量：

    ```toml Code
    SAMBANOVA_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="sambanova/Meta-Llama-3.1-8B-Instruct",
        temperature=0.7
    )
    ```
    | 模型              | 上下文窗口         | 最适用于                                     |
    |--------------------|------------------------|----------------------------------------------|
    | Llama 3.1 70B/8B   | 最多 131,072 令牌   | 高性能、大上下文任务        |
    | Llama 3.1 405B     | 8,192 令牌           | 高性能和输出质量          |
    | Llama 3.2 Series   | 8,192 令牌           | 通用、多模态任务            |
    | Llama 3.3 70B      | 最多 131,072 令牌   | 高性能和输出质量          |
    | Qwen2 familly      | 8,192 令牌           | 高性能和输出质量          |
  </Accordion>

  <Accordion title="Cerebras">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    # 必需
    CEREBRAS_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="cerebras/llama3.1-70b",
        temperature=0.7,
        max_tokens=8192
    )
    ```

    <Info>
      Cerebras 功能：
      - 快速推理速度
      - 竞争性定价
      - 速度和质量的良好平衡
      - 支持长上下文窗口
    </Info>
  </Accordion>

  <Accordion title="Open Router">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    OPENROUTER_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="openrouter/deepseek/deepseek-r1",
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY
    )
    ```

    <Info>
      Open Router 模型：
      - openrouter/deepseek/deepseek-r1
      - openrouter/deepseek/deepseek-chat
    </Info>
  </Accordion>

  <Accordion title="Nebius AI Studio">
    在您的 `.env` 文件中设置以下环境变量：
    ```toml Code
    NEBIUS_API_KEY=<your-api-key>
    ```

    在您的 CrewAI 项目中的示例用法：
    ```python Code
    llm = LLM(
        model="nebius/Qwen/Qwen3-30B-A3B"
    )
    ```

    <Info>
      Nebius AI Studio 功能：
      - 大量开源模型集合
      - 更高的速率限制
      - 竞争性定价
      - 速度和质量的良好平衡
    </Info>
  </Accordion>
</AccordionGroup>

## 流式传输响应

CrewAI 支持来自 LLMs 的流式传输响应，允许您的应用程序在生成时实时接收和处理输出。

<Tabs>
  <Tab title="基本设置">
    通过在初始化 LLM 时将 `stream` 参数设置为 `True` 来启用流式传输：

    ```python
    from crewai import LLM

    # 创建启用流式传输的 LLM
    llm = LLM(
        model="openai/gpt-4o",
        stream=True  # 启用流式传输
    )
    ```

    当启用流式传输时，响应以块形式传输，创建更响应的用户体验。
  </Tab>

  <Tab title="事件处理">
    CrewAI 为流式传输期间接收的每个块发出事件：

    ```python
    from crewai.events import (
      LLMStreamChunkEvent
    )
    from crewai.events import BaseEventListener

    class MyCustomListener(BaseEventListener):
        def setup_listeners(self, crewai_event_bus):
            @crewai_event_bus.on(LLMStreamChunkEvent)
            def on_llm_stream_chunk(self, event: LLMStreamChunkEvent):
              # 在到达时处理每个块
              print(f"接收到块：{event.chunk}")

    my_listener = MyCustomListener()
    ```

    <Tip>
      [点击此处](/zh/concepts/event-listener#event-listeners) 了解更多详情
    </Tip>
  </Tab>

  <Tab title="代理和任务跟踪">
    CrewAI 中的所有 LLM 事件都包括代理和任务信息，允许您按特定代理或任务跟踪和过滤 LLM 交互：

    ```python
    from crewai import LLM, Agent, Task, Crew
    from crewai.events import LLMStreamChunkEvent
    from crewai.events import BaseEventListener

    class MyCustomListener(BaseEventListener):
        def setup_listeners(self, crewai_event_bus):
            @crewai_event_bus.on(LLMStreamChunkEvent)
            def on_llm_stream_chunk(source, event):
                if researcher.id == event.agent_id:
                    print("\n==============\n 获得事件：", event, "\n==============\n")


    my_listener = MyCustomListener()

    llm = LLM(model="gpt-4o-mini", temperature=0, stream=True)

    researcher = Agent(
        role="关于用户",
        goal="您了解关于用户的一切。",
        backstory="""您是理解人们及其偏好的大师。""",
        llm=llm,
    )

    search = Task(
        description="回答以下关于用户的问题：{question}",
        expected_output="对问题的回答。",
        agent=researcher,
    )

    crew = Crew(agents=[researcher], tasks=[search])

    result = crew.kickoff(
        inputs={"question": "..."}
    )
    ```

    <Info>
      此功能特别适用于：
      - 调试特定代理行为
      - 按任务类型记录 LLM 使用情况
      - 审计哪些代理正在进行什么类型的 LLM 调用
      - 特定任务的性能监控
    </Info>
  </Tab>
</Tabs>

## 异步 LLM 调用

CrewAI 支持异步 LLM 调用，以提高 AI 工作流中的性能和并发性。异步调用允许您运行多个 LLM 请求而不会阻塞，使其成为高吞吐量应用程序和并行代理操作的理想选择。

<Tabs>
  <Tab title="基本用法">
    使用 `acall` 方法进行异步 LLM 请求：

    ```python
    import asyncio
    from crewai import LLM

    async def main():
        llm = LLM(model="openai/gpt-4o")

        # 单个异步调用
        response = await llm.acall("法国的首都是什么？")
        print(response)

    asyncio.run(main())
    ```

    `acall` 方法支持与同步 `call` 方法相同的所有参数，包括消息、工具和回调。
  </Tab>

  <Tab title="带流式传输">
    将异步调用与流式传输结合以实现实时并发响应：

    ```python
    import asyncio
    from crewai import LLM

    async def stream_async():
        llm = LLM(model="openai/gpt-4o", stream=True)

        response = await llm.acall("写一个关于 AI 的短故事")

        print(response)

    asyncio.run(stream_async())
    ```
  </Tab>
</Tabs>

## 结构化 LLM 调用

CrewAI 通过允许您使用 Pydantic 模型定义 `response_format` 来支持来自 LLM 调用的结构化响应。这使得框架能够自动解析和验证输出，使其更容易将响应集成到您的应用程序中而无需手动后处理。

例如，您可以定义一个 Pydantic 模型来表示预期的响应结构，并在实例化 LLM 时将其作为 `response_format` 传递。然后该模型将用于将 LLM 输出转换为结构化的 Python 对象。

```python Code
from crewai import LLM

class Dog(BaseModel):
    name: str
    age: int
    breed: str


llm = LLM(model="gpt-4o", response_format=Dog)

response = llm.call(
    "分析以下消息并返回姓名、年龄和品种。"
    "认识 Kona！她3岁，是一只黑色德国牧羊犬。"
)
print(response)

# 输出：
# Dog(name='Kona', age=3, breed='黑色德国牧羊犬')
```

## 高级功能和优化

了解如何充分利用您的 LLM 配置：

<AccordionGroup>
  <Accordion title="上下文窗口管理">
    CrewAI 包括智能上下文管理功能：

    ```python
    from crewai import LLM

    # CrewAI 自动处理：
    # 1. 令牌计数和跟踪
    # 2. 需要时的内容摘要
    # 3. 大上下文的任务分割

    llm = LLM(
        model="gpt-4",
        max_tokens=4000,  # 限制响应长度
    )
    ```

    <Info>
      上下文管理的最佳实践：
      1. 为您的任务选择合适的上下文窗口
      2. 尽可能预处理长输入
      3. 对大文档使用分块
      4. 监控令牌使用以优化成本
    </Info>
  </Accordion>

  <Accordion title="性能优化">
    <Steps>
      <Step title="令牌使用优化">
        为您的任务选择合适的上下文窗口：
        - 小任务（最多 4K 令牌）：标准模型
        - 中等任务（4K-32K 之间）：增强模型
        - 大任务（超过 32K）：大上下文模型

        ```python
        # 使用适当设置配置模型
        llm = LLM(
            model="openai/gpt-4-turbo-preview",
            temperature=0.7,    # 根据任务调整
            max_tokens=4096,    # 根据输出需求设置
            timeout=300        # 复杂任务的更长超时
        )
        ```
        <Tip>
          - 事实性响应使用较低温度（0.1 到 0.3）
          - 创意任务使用较高温度（0.7 到 0.9）
        </Tip>
      </Step>

      <Step title="最佳实践">
        1. 监控令牌使用
        2. 实施速率限制
        3. 尽可能使用缓存
        4. 设置适当的 max_tokens 限制
      </Step>
    </Steps>

    <Info>
      记住定期监控您的令牌使用情况，并根据需要调整配置以优化成本和性能。
    </Info>
  </Accordion>

  <Accordion title="丢弃额外参数">
    CrewAI 内部使用原生 sdks 进行 LLM 调用，这允许您丢弃不需要的特定用例的额外参数。这有助于简化代码并减少 LLM 配置的复杂性。
    例如，如果您不需要发送 <code>stop</code> 参数，您可以从 LLM 调用中简单地省略它：

    ```python
    from crewai import LLM
    import os

    os.environ["OPENAI_API_KEY"] = "<api-key>"

    o3_llm = LLM(
        model="o3",
        drop_params=True,
        additional_drop_params=["stop"]
    )
    ```
  </Accordion>

  <Accordion title="传输拦截器">
    CrewAI 为几个提供商提供消息拦截器，允许您在传输层挂接到请求/响应周期。

    **支持的提供商：**
    - ✅ OpenAI
    - ✅ Anthropic

    **基本用法：**
    ```python
import httpx
from crewai import LLM
from crewai.llms.hooks import BaseInterceptor

class CustomInterceptor(BaseInterceptor[httpx.Request, httpx.Response]):
    """自定义拦截器用于修改请求和响应。"""

    def on_outbound(self, request: httpx.Request) -> httpx.Request:
        """在发送到 LLM 提供商之前打印请求。"""
        print(request)
        return request

    def on_inbound(self, response: httpx.Response) -> httpx.Response:
        """在从 LLM 提供商接收后处理响应。"""
        print(f"状态：{response.status_code}")
        print(f"响应时间：{response.elapsed}")
        return response

# 将拦截器与 LLM 一起使用
llm = LLM(
    model="openai/gpt-4o",
    interceptor=CustomInterceptor()
)
    ```

    **重要说明：**
    - 两种方法都必须返回接收到的对象或对象类型。
    - 修改接收到的对象可能导致意外行为或应用程序崩溃。
    - 并非所有提供商都支持拦截器 - 请检查上面支持的提供商列表

    <Info>
      拦截器在传输层操作。这特别适用于：
      - 消息转换和过滤
      - 调试 API 交互
    </Info>
  </Accordion>
</AccordionGroup>

## 常见问题和解决方案

<Tabs>
  <Tab title="身份验证">
    <Warning>
      大多数身份验证问题可以通过检查 API 密钥格式和环境变量名称来解决。
    </Warning>

    ```bash
    # OpenAI
    OPENAI_API_KEY=sk-...

    # Anthropic
    ANTHROPIC_API_KEY=sk-ant-...
    ```
  </Tab>
  <Tab title="模型名称">
    <Check>
      始终在模型名称中包含提供商前缀
    </Check>

    ```python
    # 正确
    llm = LLM(model="openai/gpt-4")

    # 错误
    llm = LLM(model="gpt-4")
    ```
  </Tab>
  <Tab title="上下文长度">
    <Tip>
      为扩展任务使用更大的上下文模型
    </Tip>

    ```python
    # 大上下文模型
    llm = LLM(model="openai/gpt-4o")  # 128K 令牌
    ```
  </Tab>
</Tabs>